apiVersion: v1
items:
- apiVersion: controlplane.cluster.x-k8s.io/v1beta1
  kind: KubeadmControlPlane
  metadata:
    annotations:
      cluster.x-k8s.io/cloned-from-groupkind: KubeadmControlPlaneTemplate.controlplane.cluster.x-k8s.io
      cluster.x-k8s.io/cloned-from-name: nkp-nutanix-v2.14.0
    creationTimestamp: "2025-09-10T07:34:48Z"
    finalizers:
    - kubeadm.controlplane.cluster.x-k8s.io
    generation: 1
    labels:
      cluster.x-k8s.io/cluster-name: trg-nkp-mgt
      topology.cluster.x-k8s.io/owned: ""
    name: trg-nkp-mgt-x7btl
    namespace: default
    ownerReferences:
    - apiVersion: cluster.x-k8s.io/v1beta1
      blockOwnerDeletion: true
      controller: true
      kind: Cluster
      name: trg-nkp-mgt
      uid: 71a556a7-58f4-4b50-b1e8-ac0e065f4041
    resourceVersion: "97711539"
    uid: 619ec555-65d2-4f56-9291-eb07c707994f
  spec:
    kubeadmConfigSpec:
      clusterConfiguration:
        apiServer:
          certSANs:
          - 0.0.0.0
          - 127.0.0.1
          - localhost
          extraArgs:
            audit-log-maxage: "30"
            audit-log-maxbackup: "10"
            audit-log-maxsize: "100"
            audit-log-path: /var/log/audit/kube-apiserver-audit.log
            audit-policy-file: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml
            cloud-provider: external
            encryption-provider-config: /etc/kubernetes/pki/encryptionconfig.yaml
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
          extraVolumes:
          - hostPath: /etc/kubernetes/audit-policy/
            mountPath: /etc/kubernetes/audit-policy/
            name: audit-policy
            readOnly: true
          - hostPath: /var/log/kubernetes/audit
            mountPath: /var/log/audit/
            name: audit-logs
        controllerManager:
          extraArgs:
            cloud-provider: external
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
        dns:
          imageTag: v1.11.3
        etcd:
          local:
            extraArgs:
              auto-tls: "false"
              cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
              peer-auto-tls: "false"
              tls-min-version: TLS1.2
        networking: {}
        scheduler:
          extraArgs:
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
      files:
      - content: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
              - args:
                  - manager
                env:
                  - name: vip_arp
                    value: "true"
                  - name: port
                    value: '6443'
                  - name: vip_nodename
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  - name: vip_cidr
                    value: "32"
                  - name: dns_mode
                    value: first
                  - name: cp_enable
                    value: "true"
                  - name: cp_namespace
                    value: kube-system
                  - name: vip_leaderelection
                    value: "true"
                  - name: vip_leasename
                    value: plndr-cp-lock
                  - name: vip_leaseduration
                    value: "15"
                  - name: vip_renewdeadline
                    value: "10"
                  - name: vip_retryperiod
                    value: "2"
                  - name: address
                    value: '172.16.101.10'
                  - name: prometheus_server
                image: ghcr.io/kube-vip/kube-vip:v0.8.1
                imagePullPolicy: IfNotPresent
                name: kube-vip
                resources: {}
                securityContext:
                  capabilities:
                    add:
                      - NET_ADMIN
                      - NET_RAW
                volumeMounts:
                  - mountPath: /etc/kubernetes/admin.conf
                    name: kubeconfig
            hostAliases:
              - hostnames:
                  - kubernetes
                ip: 127.0.0.1
            hostNetwork: true
            volumes:
              - hostPath:
                  path: /etc/kubernetes/admin.conf
                name: kubeconfig
        owner: root:root
        path: /etc/kubernetes/manifests/kube-vip.yaml
        permissions: "0600"
      - content: |
          #!/bin/bash
          set -euo pipefail
          IFS=$'\n\t'

          SCRIPT_NAME="$(basename "${0}")"
          readonly SCRIPT_NAME

          declare -r KUBEADM_INIT_FILE="/run/kubeadm/kubeadm.yaml"
          declare -r KUBE_VIP_MANIFEST_FILE="/etc/kubernetes/manifests/kube-vip.yaml"

          function use_super_admin_conf {
            if [[ -f ${KUBEADM_INIT_FILE} && -f ${KUBE_VIP_MANIFEST_FILE} ]]; then
              sed -i 's#path: /etc/kubernetes/admin.conf#path: /etc/kubernetes/super-admin.conf#' \
                /etc/kubernetes/manifests/kube-vip.yaml
            fi
          }

          function use_admin_conf() {
            if [[ -f ${KUBEADM_INIT_FILE} && -f ${KUBE_VIP_MANIFEST_FILE} ]]; then
              sed -i 's#path: /etc/kubernetes/super-admin.conf#path: /etc/kubernetes/admin.conf#' \
                /etc/kubernetes/manifests/kube-vip.yaml
            fi
          }

          function set_host_aliases() {
            echo "127.0.0.1   kubernetes" >>/etc/hosts
          }

          function print_usage {
            cat >&2 <<EOF
          Usage: ${SCRIPT_NAME} [set-host-aliases|use-super-admin.conf|use-admin.conf]
          EOF
          }

          function run_cmd() {
            while [ $# -gt 0 ]; do
              case $1 in
              use-super-admin.conf)
                use_super_admin_conf
                ;;
              use-admin.conf)
                use_admin_conf
                ;;
              set-host-aliases)
                set_host_aliases
                ;;
              -h | --help)
                print_usage
                exit
                ;;
              *)
                echo "invalid argument"
                exit 1
                ;;
              esac
              shift
            done
          }

          run_cmd "$@"
        path: /etc/caren/configure-for-kube-vip.sh
        permissions: "0700"
      - content: |
          # Taken from https://github.com/kubernetes/kubernetes/blob/v1.28.1/cluster/gce/gci/configure-helper.sh#L1101
          # Recommended in Kubernetes docs
          apiVersion: audit.k8s.io/v1
          kind: Policy
          rules:
          # The following requests were manually identified as high-volume and low-risk,
          # so drop them.
          - level: None
            users: ["system:kube-proxy"]
            verbs: ["watch"]
            resources:
              - group: "" # core
                resources: ["endpoints", "services", "services/status"]
          - level: None
            # Ingress controller reads 'configmaps/ingress-uid' through the unsecured port.
            # TODO(#46983): Change this to the ingress controller service account.
            users: ["system:unsecured"]
            namespaces: ["kube-system"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["configmaps"]
          - level: None
            users: ["kubelet"] # legacy kubelet identity
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["nodes", "nodes/status"]
          - level: None
            userGroups: ["system:nodes"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["nodes", "nodes/status"]
          - level: None
            users:
              - system:kube-controller-manager
              - system:cloud-controller-manager
              - system:kube-scheduler
              - system:serviceaccount:kube-system:endpoint-controller
            verbs: ["get", "update"]
            namespaces: ["kube-system"]
            resources:
              - group: "" # core
                resources: ["endpoints"]
          - level: None
            users: ["system:apiserver"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
          - level: None
            users: ["cluster-autoscaler"]
            verbs: ["get", "update"]
            namespaces: ["kube-system"]
            resources:
              - group: "" # core
                resources: ["configmaps", "endpoints"]
          # Don't log HPA fetching metrics.
          - level: None
            users:
              - system:kube-controller-manager
              - system:cloud-controller-manager
            verbs: ["get", "list"]
            resources:
              - group: "metrics.k8s.io"

          # Don't log these read-only URLs.
          - level: None
            nonResourceURLs:
              - /healthz*
              - /version
              - /swagger*

          # Don't log events requests because of performance impact.
          - level: None
            resources:
              - group: "" # core
                resources: ["events"]

          # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes
          - level: Request
            users: ["kubelet", "system:node-problem-detector", "system:serviceaccount:kube-system:node-problem-detector"]
            verbs: ["update","patch"]
            resources:
              - group: "" # core
                resources: ["nodes/status", "pods/status"]
            omitStages:
              - "RequestReceived"
          - level: Request
            userGroups: ["system:nodes"]
            verbs: ["update","patch"]
            resources:
              - group: "" # core
                resources: ["nodes/status", "pods/status"]
            omitStages:
              - "RequestReceived"

          # deletecollection calls can be large, don't log responses for expected namespace deletions
          - level: Request
            users: ["system:serviceaccount:kube-system:namespace-controller"]
            verbs: ["deletecollection"]
            omitStages:
              - "RequestReceived"

          # Secrets, ConfigMaps, TokenRequest and TokenReviews can contain sensitive & binary data,
          # so only log at the Metadata level.
          - level: Metadata
            resources:
              - group: "" # core
                resources: ["secrets", "configmaps", "serviceaccounts/token"]
              - group: authentication.k8s.io
                resources: ["tokenreviews"]
            omitStages:
              - "RequestReceived"
          # Get responses can be large; skip them.
          - level: Request
            verbs: ["get", "list", "watch"]
            resources:
              - group: "" # core
              - group: "admissionregistration.k8s.io"
              - group: "apiextensions.k8s.io"
              - group: "apiregistration.k8s.io"
              - group: "apps"
              - group: "authentication.k8s.io"
              - group: "authorization.k8s.io"
              - group: "autoscaling"
              - group: "batch"
              - group: "certificates.k8s.io"
              - group: "extensions"
              - group: "metrics.k8s.io"
              - group: "networking.k8s.io"
              - group: "node.k8s.io"
              - group: "policy"
              - group: "rbac.authorization.k8s.io"
              - group: "scheduling.k8s.io"
              - group: "storage.k8s.io"
            omitStages:
              - "RequestReceived"
          # Default level for known APIs
          - level: RequestResponse
            resources:
              - group: "" # core
              - group: "admissionregistration.k8s.io"
              - group: "apiextensions.k8s.io"
              - group: "apiregistration.k8s.io"
              - group: "apps"
              - group: "authentication.k8s.io"
              - group: "authorization.k8s.io"
              - group: "autoscaling"
              - group: "batch"
              - group: "certificates.k8s.io"
              - group: "extensions"
              - group: "metrics.k8s.io"
              - group: "networking.k8s.io"
              - group: "node.k8s.io"
              - group: "policy"
              - group: "rbac.authorization.k8s.io"
              - group: "scheduling.k8s.io"
              - group: "storage.k8s.io"
            omitStages:
              - "RequestReceived"
          # Default level for all other requests.
          - level: Metadata
            omitStages:
              - "RequestReceived"
        path: /etc/kubernetes/audit-policy/apiserver-audit-policy.yaml
        permissions: "0600"
      - content: |
          #!/bin/bash
          set -euo pipefail
          IFS=$'\n\t'

          declare -r CREDENTIAL_PROVIDER_IMAGE="ghcr.io/mesosphere/dynamic-credential-provider:v0.5.3"

          if ! ctr --namespace k8s.io images check "name==${CREDENTIAL_PROVIDER_IMAGE}" | grep "${CREDENTIAL_PROVIDER_IMAGE}" >/dev/null; then
            ctr --namespace k8s.io images pull "${CREDENTIAL_PROVIDER_IMAGE}"
          fi

          cleanup() {
            ctr images unmount "${tmp_ctr_mount_dir}" || true
          }

          trap 'cleanup' EXIT

          readonly tmp_ctr_mount_dir="$(mktemp -d)"

          export CREDENTIAL_PROVIDER_SOURCE_DIR="${tmp_ctr_mount_dir}/opt/image-credential-provider/bin/"
          export CREDENTIAL_PROVIDER_TARGET_DIR="/etc/kubernetes/image-credential-provider/"

          ctr --namespace k8s.io images mount "${CREDENTIAL_PROVIDER_IMAGE}" "${tmp_ctr_mount_dir}"
          "${tmp_ctr_mount_dir}/opt/image-credential-provider/bin/dynamic-credential-provider" install
        path: /etc/caren/install-kubelet-credential-providers.sh
        permissions: "0700"
      - content: |
          apiVersion: kubelet.config.k8s.io/v1
          kind: CredentialProviderConfig
          providers:
          - name: dynamic-credential-provider
            args:
            - get-credentials
            - -c
            - /etc/kubernetes/dynamic-credential-provider-config.yaml
            matchImages:
            - "harbor.ste.training/library"
            - "*"
            - "*.*"
            - "*.*.*"
            - "*.*.*.*"
            - "*.*.*.*.*"
            - "*.*.*.*.*.*"
            defaultCacheDuration: "0s"
            apiVersion: credentialprovider.kubelet.k8s.io/v1
        path: /etc/kubernetes/image-credential-provider-config.yaml
        permissions: "0600"
      - content: |
          apiVersion: credentialprovider.d2iq.com/v1alpha1
          kind: DynamicCredentialProviderConfig
          mirror:
            endpoint: harbor.ste.training/library
            credentialsStrategy: MirrorCredentialsFirst
          credentialProviderPluginBinDir: /etc/kubernetes/image-credential-provider/
          credentialProviders:
            apiVersion: kubelet.config.k8s.io/v1
            kind: CredentialProviderConfig
            providers:
            - name: static-credential-provider
              args:
              - /etc/kubernetes/static-image-credentials.json
              matchImages:
              - "harbor.ste.training/library"
              defaultCacheDuration: "0s"
              apiVersion: credentialprovider.kubelet.k8s.io/v1
        path: /etc/kubernetes/dynamic-credential-provider-config.yaml
        permissions: "0600"
      - contentFrom:
          secret:
            key: static-credential-provider
            name: trg-nkp-mgt-static-credential-provider-response
        path: /etc/kubernetes/static-image-credentials.json
        permissions: "0600"
      - content: |
          [host."https://harbor.ste.training/v2/library"]
            capabilities = ["pull", "resolve"]
            ca = "/etc/containerd/certs.d/harbor.ste.training/ca.crt"
            # don't rely on Containerd to add the v2/ suffix
            # there is a bug where it is added incorrectly for mirrors with a path
            override_path = true
        path: /etc/containerd/certs.d/_default/hosts.toml
        permissions: "0600"
      - contentFrom:
          secret:
            key: ca.crt
            name: trg-nkp-mgt-image-registry-mirror-credentials
        path: /etc/containerd/certs.d/harbor.ste.training/ca.crt
        permissions: "0600"
      - content: |
          [plugins."io.containerd.grpc.v1.cri".registry]
            config_path = "/etc/containerd/certs.d"
        path: /etc/caren/containerd/patches/registry-config.toml
        permissions: "0600"
      - content: |
          [metrics]
            address = "0.0.0.0:1338"
            grpc_histogram = false
        path: /etc/caren/containerd/patches/metrics-config.toml
        permissions: "0600"
      - content: |
          [plugins."io.containerd.grpc.v1.cri"]
            enable_unprivileged_ports = true
            enable_unprivileged_icmp = true
        path: /etc/caren/containerd/patches/unprivileged-ports-config.toml
        permissions: "0600"
      - contentFrom:
          secret:
            key: config
            name: trg-nkp-mgt-encryption-config
        path: /etc/kubernetes/pki/encryptionconfig.yaml
        permissions: "0640"
      - content: |
          #!/bin/bash
          set -euo pipefail
          IFS=$'\n\t'

          # This script is used to merge the TOML files in the patch directory into the containerd configuration file.

          # Check if there are any TOML files in the patch directory, exiting if none are found.
          # Use a for loop that will only run a maximum of once to check if there are any files in the patch directory because
          # using -e does not work with globs.
          # See https://github.com/koalaman/shellcheck/wiki/SC2144 for an explanation of the following loop.
          patches_exist=false
          for file in "/etc/caren/containerd/patches"/*.toml; do
            if [ -e "${file}" ]; then
              patches_exist=true
            fi
            # Always break after the first iteration.
            break
          done

          if [ "${patches_exist}" = false ]; then
            echo "No TOML files found in the patch directory: /etc/caren/containerd/patches - nothing to do"
            exit 0
          fi

          # Use go template variable to avoid hard-coding the toml-merge image name in this script.
          declare -r TOML_MERGE_IMAGE="ghcr.io/mesosphere/toml-merge:v0.2.0"

          # Check if the toml-merge image is already present in ctr images list, if not pull it.
          if ! ctr --namespace k8s.io images check "name==${TOML_MERGE_IMAGE}" | grep "${TOML_MERGE_IMAGE}" >/dev/null; then
            ctr --namespace k8s.io images pull "${TOML_MERGE_IMAGE}"
          fi

          # Cleanup the temporary directory on exit.
          cleanup() {
            ctr images unmount "${tmp_ctr_mount_dir}" || true
          }
          trap 'cleanup' EXIT

          # Create a temporary directory to mount the toml-merge image filesystem.
          readonly tmp_ctr_mount_dir="$(mktemp -d)"

          # Mount the toml-merge image filesystem and run the toml-merge binary to merge the TOML files.
          ctr --namespace k8s.io images mount "${TOML_MERGE_IMAGE}" "${tmp_ctr_mount_dir}"
          "${tmp_ctr_mount_dir}/usr/local/bin/toml-merge" -i --patch-file "/etc/caren/containerd/patches/*.toml" /etc/containerd/config.toml
        path: /etc/caren/containerd/apply-patches.sh
        permissions: "0700"
      - content: |
          #!/bin/bash
          if [ "$(systemctl show containerd -p NeedDaemonReload --value)" == "yes" ]; then
            systemctl daemon-reload
          fi
          systemctl restart containerd

          if ! command -v crictl; then
            echo "Command crictl is not available, will not wait for Containerd to be running"
            exit
          fi

          SECONDS=0
          until crictl info; do
            if ((SECONDS > 60)); then
              echo "Containerd is not running. Giving up..."
              exit 1
            fi
            echo "Containerd is not running yet. Waiting..."
            sleep 5
          done
        path: /etc/caren/containerd/restart.sh
        permissions: "0700"
      format: cloud-config
      initConfiguration:
        localAPIEndpoint: {}
        nodeRegistration:
          imagePullPolicy: IfNotPresent
          kubeletExtraArgs:
            cloud-provider: external
            eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
            image-credential-provider-bin-dir: /etc/kubernetes/image-credential-provider/
            image-credential-provider-config: /etc/kubernetes/image-credential-provider-config.yaml
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
      joinConfiguration:
        discovery: {}
        nodeRegistration:
          imagePullPolicy: IfNotPresent
          kubeletExtraArgs:
            cloud-provider: external
            eviction-hard: nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<15%,memory.available<100Mi,imagefs.inodesFree<10%
            image-credential-provider-bin-dir: /etc/kubernetes/image-credential-provider/
            image-credential-provider-config: /etc/kubernetes/image-credential-provider-config.yaml
            tls-cipher-suites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
      postKubeadmCommands:
      - echo export KUBECONFIG=/etc/kubernetes/admin.conf >> /root/.bashrc
      - echo "after kubeadm call" > /var/log/postkubeadm.log
      - /bin/bash /etc/caren/configure-for-kube-vip.sh use-admin.conf
      preKubeadmCommands:
      - echo "before kubeadm call" > /var/log/prekubeadm.log
      - hostnamectl set-hostname "{{ ds.meta_data.hostname }}"
      - echo "::1         ipv6-localhost ipv6-loopback" >/etc/hosts
      - echo "127.0.0.1   localhost" >>/etc/hosts
      - echo "127.0.0.1   {{ ds.meta_data.hostname }}" >> /etc/hosts
      - /bin/bash /etc/caren/configure-for-kube-vip.sh set-host-aliases use-super-admin.conf
      - /bin/bash /etc/caren/install-kubelet-credential-providers.sh
      - /bin/bash /etc/caren/containerd/apply-patches.sh
      - /bin/bash /etc/caren/containerd/restart.sh
      useExperimentalRetryJoin: true
      users:
      - lockPassword: true
        name: konvoy
        sshAuthorizedKeys:
        - |
          ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC7RJwopRlGG6VZ92mDDfQSur6oyBosmsMGJ8CScob0X4hO8t0dvIBPHPA90N88O8naSPVojBWmy6jDkl3uu2RYySj7dji8iTl3lvfKNmSZdFQqPCIJM34tfTxCm1dHHHrAgb+MaERlOJNF7aS6bxZdiUYZvQitlZ4b9gRelWA4Yoi8rYPWp95KwqfDJdFmAa70NyNERidGhv3+HEqJCJC4AwR58cxiXzT0HCgrVFFhDRSokmocnM4TFCNbniuICpRFbMo6b+LXhO4ActF3BWKYPxLrNJjNgTFeMh1IHhEo50gTLNGOyzOFXz7mzaivsCwMGEVmDE5r+pzBSIaHkFeyhgdMvmPmsfNwEAogMPCM/WHDAkMQuY/nULX0+YO9Q2jaQolTfwylnY4aAC9T2aTb1jVaQ4XlrpxnHwMEN2PdKI9qFr1xicdPaz80arr1lcDQcDLwn6UK9BU31L0EQr1HAvsYeKIQVaz7i3w4N4CN7wt4+6N+BTdFHHd4pgOzd58=
        sudo: ALL=(ALL) NOPASSWD:ALL
      verbosity: 10
    machineTemplate:
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: NutanixMachineTemplate
        name: trg-nkp-mgt-sw22h
        namespace: default
      metadata:
        labels:
          cluster.x-k8s.io/cluster-name: trg-nkp-mgt
          topology.cluster.x-k8s.io/owned: ""
    replicas: 3
    rolloutStrategy:
      rollingUpdate:
        maxSurge: 1
      type: RollingUpdate
    version: v1.31.4
  status:
    conditions:
    - lastTransitionTime: "2025-09-10T07:34:57Z"
      status: "True"
      type: Ready
    - lastTransitionTime: "2025-09-10T07:34:56Z"
      status: "True"
      type: Available
    - lastTransitionTime: "2025-09-10T07:34:55Z"
      status: "True"
      type: CertificatesAvailable
    - lastTransitionTime: "2025-09-10T07:34:57Z"
      status: "True"
      type: ControlPlaneComponentsHealthy
    - lastTransitionTime: "2025-10-15T19:45:22Z"
      status: "True"
      type: EtcdClusterHealthy
    - lastTransitionTime: "2025-09-10T07:34:56Z"
      status: "True"
      type: MachinesCreated
    - lastTransitionTime: "2025-09-10T07:34:57Z"
      status: "True"
      type: MachinesReady
    - lastTransitionTime: "2025-09-10T07:34:57Z"
      status: "True"
      type: Resized
    initialized: true
    observedGeneration: 1
    ready: true
    readyReplicas: 3
    replicas: 3
    selector: cluster.x-k8s.io/cluster-name=trg-nkp-mgt,cluster.x-k8s.io/control-plane
    updatedReplicas: 3
    v1beta2:
      availableReplicas: 0
      conditions:
      - lastTransitionTime: "2025-10-10T03:05:50Z"
        message: ""
        observedGeneration: 1
        reason: Available
        status: "True"
        type: Available
      - lastTransitionTime: "2025-09-10T07:34:55Z"
        message: ""
        observedGeneration: 1
        reason: Available
        status: "True"
        type: CertificatesAvailable
      - lastTransitionTime: "2025-09-10T07:34:56Z"
        message: ""
        observedGeneration: 1
        reason: Initialized
        status: "True"
        type: Initialized
      - lastTransitionTime: "2025-10-16T01:09:09Z"
        message: ""
        observedGeneration: 1
        reason: Healthy
        status: "True"
        type: EtcdClusterHealthy
      - lastTransitionTime: "2025-09-10T07:34:57Z"
        message: ""
        observedGeneration: 1
        reason: Healthy
        status: "True"
        type: ControlPlaneComponentsHealthy
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: NotRollingOut
        status: "False"
        type: RollingOut
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: NotRemediating
        status: "False"
        type: Remediating
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: NotScalingDown
        status: "False"
        type: ScalingDown
      - lastTransitionTime: "2025-09-10T07:34:56Z"
        message: ""
        observedGeneration: 1
        reason: NotScalingUp
        status: "False"
        type: ScalingUp
      - lastTransitionTime: "2025-09-10T07:34:57Z"
        message: |-
          * Machines trg-nkp-mgt-x7btl-4sptp, trg-nkp-mgt-x7btl-qhlqc, trg-nkp-mgt-x7btl-r7cvn:
            * BootstrapConfigReady:
              * CertificatesAvailable: Condition not yet reported
        observedGeneration: 1
        reason: ReadyUnknown
        status: Unknown
        type: MachinesReady
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: UpToDate
        status: "True"
        type: MachinesUpToDate
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: NotPaused
        status: "False"
        type: Paused
      - lastTransitionTime: "2025-09-10T07:34:54Z"
        message: ""
        observedGeneration: 1
        reason: NotDeleting
        status: "False"
        type: Deleting
      readyReplicas: 0
      upToDateReplicas: 3
    version: v1.31.4
kind: List
metadata:
  resourceVersion: ""
